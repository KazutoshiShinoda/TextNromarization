{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(window=1, min_count=2)\n",
    "\n",
    "sentences = [[\"a\",\"b\",\"c\",\"a\",\"b\",\"a\",\"b\",\"b\"],[\"a\",\"a\",\"a\"]]\n",
    "model.build_vocab(sentences, keep_raw_vocab=True)\n",
    "sentences = [[\"d\",\"d\",\"d\"]*10,[\"d\",\"a\",\"a\"]*10+[\"c\"]]\n",
    "model.build_vocab(sentences, keep_raw_vocab=True, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': <gensim.models.keyedvectors.Vocab at 0x7ff737851b70>,\n",
       " 'b': <gensim.models.keyedvectors.Vocab at 0x7ff7378512e8>,\n",
       " 'd': <gensim.models.keyedvectors.Vocab at 0x7ff737851a20>}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(window=1, min_count=2)\n",
    "\n",
    "sentences = [[\"a\",\"b\",\"c\",\"a\",\"b\",\"a\",\"b\",\"b\"],[\"a\",\"a\",\"a\"]]\n",
    "model.build_vocab(sentences, keep_raw_vocab=True)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': <gensim.models.keyedvectors.Vocab at 0x7ff7378515c0>,\n",
       " 'b': <gensim.models.keyedvectors.Vocab at 0x7ff737845f28>}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 40, 'b': 10, 'c': 10})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.raw_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00088881073"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"a\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [[\"d\",\"d\",\"d\"]*10,[\"d\",\"a\",\"a\"]*10+[\"c\"]]\n",
    "model.build_vocab(sentences, keep_raw_vocab=True, update=True)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00081965426"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"a\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037984129"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"d\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': <gensim.models.keyedvectors.Vocab at 0x7ff7378515c0>,\n",
       " 'b': <gensim.models.keyedvectors.Vocab at 0x7ff737845f28>,\n",
       " 'd': <gensim.models.keyedvectors.Vocab at 0x7ff737851128>}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 20, 'd': 40})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.raw_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_word2vec.Word2Vec(size=128, window=5, min_count=1, keep_raw_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 3 1\n",
      "a 3\n",
      "c 3 1\n",
      "c 3\n",
      "a {'a': <gensim.models.keyedvectors.Vocab object at 0x7f2341b0f198>, 'c': <gensim.models.keyedvectors.Vocab object at 0x7f22784b41d0>} False\n",
      "2 defaultdict(<class 'int'>, {'a': 3, 'c': 3})\n",
      "4 defaultdict(<class 'int'>, {'a': 3, 'c': 3})\n",
      "6 defaultdict(<class 'int'>, {'a': 3, 'c': 3})\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab([[\"c\",\"c\",\"c\",\"a\"],[\"a\"],[\"a\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010310465"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"a\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 defaultdict(<class 'int'>, {'b': 1, 'a': 4, 'c': 4})\n",
      "7 defaultdict(<class 'int'>, {'b': 1, 'a': 4, 'c': 4})\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab([[\"b\",\"a\",\"c\"]], update=True, n_build=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010310465"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"a\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017000992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"b\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train([[\"a\", \"c\"]*100,[\"a\"]*100,[\"c\"]*100], total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0073690359"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"a\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train([[\"a\", \"b\"]*100,[\"b\"]*100,[\"c\"]*100], total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "INPUT_PATH = \"../../input/\"\n",
    "OUTPUT_PATH = \"../../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['output_1.csv', 'output_6.csv', 'output_11.csv', 'output_16.csv',\n",
    "         'output_21.csv', 'output_91.csv', 'output_96.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 100000 sentences. Now in output_1.csv\n",
      "Trained 200000 sentences. Now in output_1.csv\n",
      "Trained 300000 sentences. Now in output_1.csv\n",
      "Trained 400000 sentences. Now in output_1.csv\n",
      "Trained 500000 sentences. Now in output_1.csv\n",
      "Trained 600000 sentences. Now in output_1.csv\n",
      "Trained 700000 sentences. Now in output_1.csv\n",
      "Trained 800000 sentences. Now in output_1.csv\n",
      "Trained 900000 sentences. Now in output_1.csv\n",
      "Trained 1000000 sentences. Now in output_1.csv\n",
      "Trained 1100000 sentences. Now in output_1.csv\n",
      "Trained 1200000 sentences. Now in output_1.csv\n",
      "Trained 1300000 sentences. Now in output_1.csv\n",
      "Trained 1400000 sentences. Now in output_1.csv\n",
      "Trained 1500000 sentences. Now in output_1.csv\n",
      "Trained 1600000 sentences. Now in output_1.csv\n",
      "Trained 1700000 sentences. Now in output_1.csv\n",
      "Trained 1800000 sentences. Now in output_1.csv\n",
      "Trained 1900000 sentences. Now in output_1.csv\n",
      "Trained 2000000 sentences. Now in output_1.csv\n",
      "Trained 2100000 sentences. Now in output_1.csv\n",
      "Trained 2200000 sentences. Now in output_1.csv\n",
      "Trained 2300000 sentences. Now in output_1.csv\n",
      "Trained 2400000 sentences. Now in output_1.csv\n",
      "Trained 2500000 sentences. Now in output_1.csv\n",
      "Trained 2600000 sentences. Now in output_1.csv\n",
      "Trained 2700000 sentences. Now in output_1.csv\n",
      "Trained 2800000 sentences. Now in output_1.csv\n",
      "Trained 2900000 sentences. Now in output_1.csv\n",
      "Trained 3000000 sentences. Now in output_1.csv\n",
      "Trained 3100000 sentences. Now in output_1.csv\n",
      "Trained 3200000 sentences. Now in output_1.csv\n",
      "Trained 3300000 sentences. Now in output_1.csv\n",
      "Trained 3400000 sentences. Now in output_1.csv\n",
      "Trained 3500000 sentences. Now in output_1.csv\n",
      "Trained 3600000 sentences. Now in output_1.csv\n",
      "Trained 3700000 sentences. Now in output_1.csv\n",
      "Trained 3800000 sentences. Now in output_1.csv\n",
      "Trained 3900000 sentences. Now in output_1.csv\n",
      "Trained 4000000 sentences. Now in output_1.csv\n",
      "Trained 4100000 sentences. Now in output_1.csv\n",
      "Trained 4200000 sentences. Now in output_1.csv\n",
      "Trained 4300000 sentences. Now in output_1.csv\n",
      "Trained 4400000 sentences. Now in output_1.csv\n",
      "Trained 4500000 sentences. Now in output_6.csv\n",
      "Trained 4600000 sentences. Now in output_6.csv\n",
      "Trained 4700000 sentences. Now in output_6.csv\n",
      "Trained 4800000 sentences. Now in output_6.csv\n",
      "Trained 4900000 sentences. Now in output_6.csv\n",
      "Trained 5000000 sentences. Now in output_6.csv\n",
      "Trained 5100000 sentences. Now in output_6.csv\n",
      "Trained 5200000 sentences. Now in output_6.csv\n",
      "Trained 5300000 sentences. Now in output_6.csv\n",
      "Trained 5400000 sentences. Now in output_6.csv\n",
      "Trained 5500000 sentences. Now in output_6.csv\n",
      "Trained 5600000 sentences. Now in output_6.csv\n",
      "Trained 5700000 sentences. Now in output_6.csv\n",
      "Trained 5800000 sentences. Now in output_6.csv\n",
      "Trained 5900000 sentences. Now in output_6.csv\n",
      "Trained 6000000 sentences. Now in output_6.csv\n",
      "Trained 6100000 sentences. Now in output_6.csv\n",
      "Trained 6200000 sentences. Now in output_6.csv\n",
      "Trained 6300000 sentences. Now in output_6.csv\n",
      "Trained 6400000 sentences. Now in output_6.csv\n",
      "Trained 6500000 sentences. Now in output_6.csv\n",
      "Trained 6600000 sentences. Now in output_6.csv\n",
      "Trained 6700000 sentences. Now in output_6.csv\n",
      "Trained 6800000 sentences. Now in output_6.csv\n",
      "Trained 6900000 sentences. Now in output_6.csv\n",
      "Trained 7000000 sentences. Now in output_6.csv\n",
      "Trained 7100000 sentences. Now in output_6.csv\n",
      "Trained 7200000 sentences. Now in output_6.csv\n",
      "Trained 7300000 sentences. Now in output_6.csv\n",
      "Trained 7400000 sentences. Now in output_6.csv\n",
      "Trained 7500000 sentences. Now in output_6.csv\n",
      "Trained 7600000 sentences. Now in output_6.csv\n",
      "Trained 7700000 sentences. Now in output_6.csv\n",
      "Trained 7800000 sentences. Now in output_6.csv\n",
      "Trained 7900000 sentences. Now in output_6.csv\n",
      "Trained 8000000 sentences. Now in output_6.csv\n",
      "Trained 8100000 sentences. Now in output_6.csv\n",
      "Trained 8200000 sentences. Now in output_6.csv\n",
      "Trained 8300000 sentences. Now in output_6.csv\n",
      "Trained 8400000 sentences. Now in output_6.csv\n",
      "Trained 8500000 sentences. Now in output_6.csv\n",
      "Trained 8600000 sentences. Now in output_6.csv\n",
      "Trained 8700000 sentences. Now in output_6.csv\n",
      "Trained 8800000 sentences. Now in output_6.csv\n",
      "Trained 8900000 sentences. Now in output_11.csv\n",
      "Trained 9000000 sentences. Now in output_11.csv\n",
      "Trained 9100000 sentences. Now in output_11.csv\n",
      "Trained 9200000 sentences. Now in output_11.csv\n",
      "Trained 9300000 sentences. Now in output_11.csv\n",
      "Trained 9400000 sentences. Now in output_11.csv\n",
      "Trained 9500000 sentences. Now in output_11.csv\n",
      "Trained 9600000 sentences. Now in output_11.csv\n",
      "Trained 9700000 sentences. Now in output_11.csv\n",
      "Trained 9800000 sentences. Now in output_11.csv\n",
      "Trained 9900000 sentences. Now in output_11.csv\n",
      "Trained 10000000 sentences. Now in output_11.csv\n",
      "Trained 10100000 sentences. Now in output_11.csv\n",
      "Trained 10200000 sentences. Now in output_11.csv\n",
      "Trained 10300000 sentences. Now in output_11.csv\n",
      "Trained 10400000 sentences. Now in output_11.csv\n",
      "Trained 10500000 sentences. Now in output_11.csv\n",
      "Trained 10600000 sentences. Now in output_11.csv\n",
      "Trained 10700000 sentences. Now in output_11.csv\n",
      "Trained 10800000 sentences. Now in output_11.csv\n",
      "Trained 10900000 sentences. Now in output_11.csv\n",
      "Trained 11000000 sentences. Now in output_11.csv\n",
      "Trained 11100000 sentences. Now in output_11.csv\n",
      "Trained 11200000 sentences. Now in output_11.csv\n",
      "Trained 11300000 sentences. Now in output_11.csv\n",
      "Trained 11400000 sentences. Now in output_11.csv\n",
      "Trained 11500000 sentences. Now in output_11.csv\n",
      "Trained 11600000 sentences. Now in output_11.csv\n",
      "Trained 11700000 sentences. Now in output_11.csv\n",
      "Trained 11800000 sentences. Now in output_11.csv\n",
      "Trained 11900000 sentences. Now in output_11.csv\n",
      "Trained 12000000 sentences. Now in output_11.csv\n",
      "Trained 12100000 sentences. Now in output_11.csv\n",
      "Trained 12200000 sentences. Now in output_11.csv\n",
      "Trained 12300000 sentences. Now in output_11.csv\n",
      "Trained 12400000 sentences. Now in output_11.csv\n",
      "Trained 12500000 sentences. Now in output_11.csv\n",
      "Trained 12600000 sentences. Now in output_11.csv\n",
      "Trained 12700000 sentences. Now in output_11.csv\n",
      "Trained 12800000 sentences. Now in output_11.csv\n",
      "Trained 12900000 sentences. Now in output_11.csv\n",
      "Trained 13000000 sentences. Now in output_11.csv\n",
      "Trained 13100000 sentences. Now in output_11.csv\n",
      "Trained 13200000 sentences. Now in output_11.csv\n",
      "Trained 13300000 sentences. Now in output_16.csv\n",
      "Trained 13400000 sentences. Now in output_16.csv\n",
      "Trained 13500000 sentences. Now in output_16.csv\n",
      "Trained 13600000 sentences. Now in output_16.csv\n",
      "Trained 13700000 sentences. Now in output_16.csv\n",
      "Trained 13800000 sentences. Now in output_16.csv\n",
      "Trained 13900000 sentences. Now in output_16.csv\n",
      "Trained 14000000 sentences. Now in output_16.csv\n",
      "Trained 14100000 sentences. Now in output_16.csv\n"
     ]
    }
   ],
   "source": [
    "    data = []\n",
    "    n_sen=0\n",
    "    is_first = True\n",
    "    for file in files:\n",
    "        source = open(INPUT_PATH + file, encoding='UTF8')\n",
    "        line = source.readline()\n",
    "        sentence = []\n",
    "        while 1:\n",
    "            line = source.readline().strip()\n",
    "            if line == '':\n",
    "                break\n",
    "            line = line.replace(',NA,', ',\"NA\",')\n",
    "            pos = line.find('\",\"')\n",
    "            text = line[pos + 2:]\n",
    "            if text[:3] == '\",\"':\n",
    "                #カンマは無視\n",
    "                continue\n",
    "            text = text[1:-1]\n",
    "            arr = text.split('\",\"')\n",
    "            if arr[0].isdigit():\n",
    "                if len(arr[0])==4:\n",
    "                    arr[0] = \"<YEAR>\"\n",
    "                elif len(arr[0])==3:\n",
    "                    arr[0] = \"<3_DIGIT>\"\n",
    "                elif len(arr[0])==2:\n",
    "                    arr[0] = \"<2_DIGIT>\"\n",
    "            if arr[0] != '<eos>':\n",
    "                sentence.append(arr[0])\n",
    "            else:\n",
    "                data.append(sentence)\n",
    "                if len(data) == 10000:\n",
    "                    if is_first:\n",
    "                        model = word2vec.Word2Vec(data, size=128, window=5, min_count=1, workers=4)\n",
    "                    else:\n",
    "                        model.train(data)\n",
    "                    n_sen += 10000\n",
    "                    if n_sen % 100000 == 0:\n",
    "                        print(\"\\rTrained %i sentences. Now in %s\" % (n_sen, file))\n",
    "                    data=[]\n",
    "                sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from TNLineSentences import TNLineSentences as LS\n",
    "from TNLineSentences import TNLineSentencesST as LSST\n",
    "from config import INPUT_PATH, OUTPUT_PATH\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(OUTPUT_PATH+'embeddings_0.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-51d83e1cf30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/app/pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/pyenv/versions/3.4.3/lib/python3.4/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "\n",
    "model.wv[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89021528,  3.17404819,  3.83192325, -0.47394833,  1.26843798,\n",
       "       -0.91536456,  0.33848202,  1.94089878, -3.4787035 , -2.44208646,\n",
       "        0.2697857 , -1.36734295, -0.26015174,  1.35134661, -1.64302516,\n",
       "       -2.10293818,  0.62881088,  0.25294772,  2.23715544, -1.17393816,\n",
       "        0.77953392, -0.12971996,  2.30466866,  2.36578178,  1.39669204,\n",
       "       -1.02168357,  2.12357593,  1.28572464,  2.80658531, -1.74055111,\n",
       "        1.5931375 , -2.92859459,  0.81227493,  0.04677116, -0.43147507,\n",
       "        1.11092567, -2.16464734,  0.54993206,  1.33714712,  0.66074663,\n",
       "       -0.53001571, -0.6132006 , -0.21540926, -0.13413773,  0.37317619,\n",
       "        2.9303925 , -0.73703235, -1.12137592, -1.01806045, -3.90592527,\n",
       "       -3.73420835, -1.28138494,  1.38598466,  0.98047119, -2.72553992,\n",
       "       -4.10574055,  1.36095619, -0.25975248,  1.03923392, -0.73190767,\n",
       "        2.66475463, -0.32220298,  1.31529236, -2.10021544, -0.79568583,\n",
       "        1.3713845 , -1.97418535, -3.09236455,  1.22170591, -3.01326418,\n",
       "       -1.08106947, -0.82314157,  0.56067681, -1.34754097, -4.69130707,\n",
       "        0.07506486,  0.0285256 , -1.10281813,  0.81813884,  1.87652588,\n",
       "        0.37304544, -0.41402167, -0.64120543, -2.90931344,  0.36588627,\n",
       "        0.15854537, -2.32445025, -0.60264844, -2.55882263, -2.56613159,\n",
       "        3.07290363, -1.96339393, -0.17775986,  2.26226807,  2.06720638,\n",
       "       -1.08988965,  2.16674352, -0.25838521, -2.95273972,  1.01390576,\n",
       "       -1.98246324,  3.84921527,  2.42767286,  1.86580265, -1.18258083,\n",
       "       -0.63882864,  1.51312351, -3.25857854,  1.66055036,  1.53784931,\n",
       "       -2.69289112,  3.1650753 ,  1.47562182, -0.04589288, -0.95540261,\n",
       "        0.60344625,  0.0859254 , -4.00229216, -3.86712193,  0.41104311,\n",
       "        1.25525391, -0.64783359,  0.18524848, -5.48416281,  4.21616697,\n",
       "       -2.15731978,  0.89754725,  4.08445978], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"—\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.wv[\"<YEAR>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.concatenate([v,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LSST(INPUT_PATH+'dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file %s ../../input/dataset/output_1.csv\n",
      "— sil\n",
      "['Greek', 'National', 'Road', '<2_DIGIT>', '(', 'Athens', '—', 'Sounio', ')', 'passes', 'through', 'the', 'town', '.']\n",
      "- sil\n",
      "['\"\"', '<YEAR>', 'ALCS', 'Game', '2', '-', 'Detroit', 'Tigers', 'vs']\n",
      "- to\n",
      "['Thus', 'during', 'the', 'years', '<YEAR>', '-', '<YEAR>', 'they', 'rebuilt', 'the', 'old', 'castle', 'just', 'like', 'many', 'other', 'castles', 'throughout', 'the', 'island', '.']\n",
      "- to\n",
      "['The', 'Standard', 'Catalog', 'of', 'American', 'Cars', '<YEAR>', '-', '<YEAR>', '.']\n",
      "- sil\n",
      "['\"\"', 'Jungle', 'Stampede', '(', '<YEAR>', ')', '-', 'Overview', '\"\"', '.']\n",
      "- to\n",
      "['dr', 'Henry', 'Peckwell', '(', '<YEAR>', '-', '<YEAR>', ')', 'and', 'Bella', 'Blosset', 'of', 'Co', '.']\n",
      "— sil\n",
      "['\"\"', 'United', 'Nations', 'Statistics', 'Division', '—', 'Demographic', 'and', 'Social', 'Statistics', '\"\"', '.']\n",
      "— sil\n",
      "['Sol', 'Plaatje', '—', 'the', 'Tswana', 'author', 'and', 'first', 'secretary', 'of', 'the', 'African', 'National', 'Congress', '.']\n",
      "- to\n",
      "['Archives', 'des', 'Sciences', 'Naturelles', 'Physiques', 'et', 'Mathematiques', '<2_DIGIT>', '<2_DIGIT>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['See', '\"\"', 'BEA', ':', 'CA', '1', '-3', '-', 'Per', 'capita', 'personal', 'income', '\"\"', '.']\n",
      "- to\n",
      "['<2_DIGIT>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['\"\"', 'Iowa', 'Aviation', 'Hall', 'of', 'Fame', ':', 'Inductees', '-', '<YEAR>', '-', 'Robert', 'L.', 'Taylor', '.', '\"\"']\n",
      "- to\n",
      "['She', 'married', 'John', 'Pitt', 'of', \"Crow's\", 'Hall', 'Debenham', 'Suffolk', 'brother', 'of', 'George', 'Pitt', '(', '<YEAR>', '-', '<YEAR>', ')', '.']\n",
      "- sil\n",
      "['It', 'can', 'eject', 'the', 'F', '-', 'armor', 'to', 'revert', 'to', 'the', 'MBV-', '<3_DIGIT>', 'T', '.']\n",
      "- to\n",
      "['Revista', 'de', 'Istorie', 'Militara', 'nr', '.', '1/2007', 'p', '.', '<2_DIGIT>', '-', '<3_DIGIT>', 'Ion', 'Constantin', '\"\"', 'Chisinau', '<YEAR>', '-', '<YEAR>', '.']\n",
      "— sil\n",
      "['\"\"', 'Sure', 'Love', '—', 'Single', '\"\"', '.']\n",
      "- to\n",
      "['The', 'Veliger', '<2_DIGIT>', '(', '1', ')', ':', '<3_DIGIT>', '-', '<3_DIGIT>', '.']\n",
      "- to\n",
      "['ACM', '<2_DIGIT>', '3', '(', 'March 2013', ')', '<2_DIGIT>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['Olin', 'Edirne', 'Basket', \"haven't\", 'got', 'a', 'good', 'season', 'in', '<YEAR>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['It', 'can', 'be', 'considered', 'a', 'spiritual', 'home', 'console', 'version', 'of', 'the', '4', '-', 'player', 'Force', '.']\n",
      "- to\n",
      "['The', 'paintings', 'of', 'the', \"America's\", 'cup', '<YEAR>', '-', '<YEAR>', 'text', 'by', 'Ranulf', 'Rayner', ';', 'paintings', 'by', 'Tim', 'Thompson', ';', 'forward', 'by', 'Ted', 'Turner', '.']\n",
      "- to\n",
      "['Bob', 'Dylan', ':', 'The', 'Recording', 'Sessions', '<YEAR>', '-', '<YEAR>', '.']\n",
      "— sil\n",
      "['\"\"', 'Serbia', '—', 'People', 'Groups', '.']\n",
      "— sil\n",
      "['\"\"', 'Discogs', ':', 'Nik', 'Kershaw', '—', 'Human', 'Racing', '7', '\"\"', 'Cover', 'Images', '\"\"', '.']\n",
      "— sil\n",
      "['Landesbetrieb', 'fur', 'Statistik', 'und', 'Kommunikationstechnologie', 'Niedersachsen', '<3_DIGIT>', 'Bevolkerung', '—', 'Basis', 'Zensus', '<YEAR>', 'Stand', '<2_DIGIT>', '.']\n",
      "- to\n",
      "['Proc', 'Zool', 'Soc', 'Lond', '<3_DIGIT>', '-', '<3_DIGIT>', '.']\n",
      "— sil\n",
      "['The', 'Trailblazer', 'gets', 'a', 'Marginal', 'overall', 'side', 'impact', 'rating', '—', 'tested', 'with', 'optional', 'side', 'airbags', 'present', '.']\n",
      "- sil\n",
      "['The', 'Koreas', 'p', '.', '<2_DIGIT>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['\"\"', 'Central', 'Statistical', 'Office', '(', 'GUS', ')', '-', 'TERYT', '(', 'National', 'Register', 'of', 'Territorial', 'Land', 'Apportionment', 'Journal', ')', '\"\"', '(', 'in', 'Polish', ')', '.']\n",
      "- to\n",
      "['Georgia', 'had', 'made', 'it', 'to', '8', '-', '0', 'when', 'coach', \"Dooley's\", 'Bulldogs', 'faced', 'the', 'most', 'daunting', 'task', 'of', 'the', 'year', '.']\n",
      "- to\n",
      "['Women', 'in', 'Judaism', ':', 'A', 'Multidisciplinary', 'Journal', '(', 'ISSN', '<YEAR>', '-', '<YEAR>', ')', '.']\n",
      "- sil\n",
      "['The', 'same', 'year', 'he', 'appeared', 'on', 'the', 'Richard', 'X', 'album', 'Richard', 'X', 'Presents', 'His', 'X', '-', 'Factor', 'vol', '1', '.']\n",
      "— sil\n",
      "['\"\"', '74th', 'Academy', 'Awards', '—', 'Nominees', 'and', 'Winners', '\"\"', '.']\n",
      "- sil\n",
      "['Encyclopaedia', 'Metallum', ':', 'Debra', 'Armstrong', '\"\"', 'Guitarist', 'MICHELLE', 'MELDRUM', 'Passes', 'Away', 'At', 'Age', '<2_DIGIT>', '-', 'May 22, 2008', '\"\"', '.']\n",
      "- sil\n",
      "['ISBN', '0-89879-823', '-', 'X', '.', 'James', 'George', '(', '24 August 1994', ')', '.']\n",
      "- sil\n",
      "['DOS', '<3_DIGIT>', '-', 'W', '<2_DIGIT>', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['Philp', 'became', 'member', 'for', 'Townsville', 'in', '<YEAR>', 'Premier', 'in', 'December 1899', 'and', 'Premier', 'again', 'in', '<YEAR>', '-', '<YEAR>', '.']\n",
      "— sil\n",
      "['McGill', \"Queen's\", 'Press', '—', 'MQUP', '.']\n",
      "- to\n",
      "['He', 'served', 'three', 'separate', 'terms', 'as', 'the', 'Executive', 'Secretary', 'of', 'the', 'USFA', ';', '<YEAR>', '-', '<YEAR>', '<YEAR>', '-', '<YEAR>', 'and', 'a', 'final', 'term', 'in', '<YEAR>', '.']\n",
      "- sil\n",
      "['There', 'is', 'a', 'media', 'room', 'which', 'shows', 'the', '<2_DIGIT>', '-', 'minute', 'Home', 'Box', 'Office', 'documentary', 'film', 'Dear', 'America', ':', 'Letters', 'Home', 'from', 'Vietnam', '.']\n",
      "- sil\n",
      "['He', 'was', 'also', 'named', 'to', 'the', '<YEAR>', '-', '<2_DIGIT>', 'All', 'Rookie', 'first', 'team', '.']\n",
      "- sil\n",
      "['Ellis', 'later', 'on', 'struggled', 'with', 'injuries', 'only', 'playing', 'six', 'games', 'in', 'the', '<YEAR>', '-', '<2_DIGIT>', 'season', '.']\n",
      "- to\n",
      "['From', '<YEAR>', '-', '<YEAR>', 'he', 'was', 'headmaster', 'of', 'a', 'preparatory', 'school', 'in', 'Farnborough', 'Hampshire', '.']\n",
      "- sil\n",
      "['<YEAR>', '-', 'Representing', 'cpl']\n",
      "— sil\n",
      "['\"\"', 'Syrian', 'opposition', 'groups', 'sign', 'coalition', 'deal', '—', 'Middle', 'East', '\"\"', '.']\n",
      "- sil\n",
      "['\"\"', 'NBA', 'Development', 'League', ':', 'D', '-', 'Fenders', 'Acquire', 'Jarvis', 'Varnado', '\"\"', '.']\n",
      "— sil\n",
      "['\"\"', 'The', 'Monsters', 'of', 'Gaming', '—', 'IGN', '\"\"', '.']\n",
      "- sil\n",
      "['Profound', 'cytotoxicity', 'of', 'SWNTs', 'was', 'observed', 'in', 'alveolar', 'macrophage', '(', 'AM', ')', 'after', 'a', '6', '-', 'hour', 'exposure', 'in', 'vitro', '.']\n",
      "- sil\n",
      "['\"\"', 'Metabolism', 'of', 'Styrene', 'Oxide', 'and', '2', '-', 'Phenylethanol', 'in', 'the', 'Styrene', 'Degrading', 'Xanthobacter', 'Strain', '<3_DIGIT>', 'X', '\"\"', '.']\n",
      "— sil\n",
      "['Centre', 'for', 'Accident', 'Research', 'and', 'Road', 'Safety', '—', 'Queensland', '\"\"', 'CARRS', 'Q', ':', 'Road', 'Safety', 'Training', 'and', 'Education', '\"\"', '<YEAR>', '.']\n",
      "- to\n",
      "['Coromandel', 'Rudolf', 'Steiner', 'School', 'was', 'a', 'small', 'private', 'full', 'primary', '(', 'years', '1', '-', '8', ')', 'school', '.']\n",
      "- sil\n",
      "['\"\"', 'Eva', 'Green', 'Heads', 'to', '\"\"', 'Sin', 'City', ':', 'A', 'Dame', 'to', 'Kill', 'For', '\"\"', '-', 'Comic', 'Book', 'Resources', '\"\"', '.']\n",
      "- sil\n",
      "['\"\"', 'Deafen', 'County', '—', 'NICK', 'WATERHOUSE', '—', 'LONDON', '-', '10.04.2014', '\"\"', '.']\n",
      "— sil\n",
      "['GM', 'may', 'redesign', 'the', 'Volt', 'battery', '—', 'CEO', 'Akerson', '(', 'Television', 'news', ')', '.']\n",
      "- sil\n",
      "['ISBN', '1-84530-019', '-', 'X', '.', 'p', '.', '<2_DIGIT>', 'Love', 'Dane', '(', '<YEAR>', ')', 'Ayrshire', ':', 'Discovering', 'a', 'County', '.']\n",
      "— sil\n",
      "['CBS', '—', 'Population', 'counter', '—', 'Extra', '\"\"', 'The', 'World', 'Factbook', '\"\"', '.']\n",
      "— sil\n",
      "['\"\"', 'Final', 'communiqué', 'of', 'the', 'Action', 'Group', 'for', 'Syria', '—', 'Geneva', 'Saturday 30 June 2012', '\"\"', '.']\n",
      "- to\n",
      "['Adjutant', 'General', 'Report', '<YEAR>', '-', '<YEAR>', '.']\n",
      "- to\n",
      "['ISBN', '0', '-', '<3_DIGIT>', '-', '77152-', 'X', '.', '-', 'Total', 'pages', ':', '<3_DIGIT>', 'Fay', 'John', '(', '<YEAR>', ')', '.']\n",
      "- sil\n",
      "['ISBN', '3-89639-522', '-', 'X', '.', 'Layer', 'Adolf', '(', 's', '.', 'a', '.', ')', '.']\n",
      "- to\n",
      "['S', '.', 'I', 'VII', 'S', '.1', '-', '<3_DIGIT>', 'Taf', '.1', '-', '<2_DIGIT>', 'Paris', 'Bailliere', '.']\n",
      "- to\n",
      "['He', 'was', 'portrayed', 'by', 'Sarai', 'Tzuriel', 'in', '<YEAR>', '-', '<YEAR>', 'and', 'later', 'by', 'Guy', 'Friedman', 'in', '<YEAR>', '.']\n",
      "- to\n",
      "['<3_DIGIT>', '-', '<3_DIGIT>', 'Article', '<3_DIGIT>', 'para', '.']\n",
      "- sil\n",
      "['Eventually', 'the', 'L', '<2_DIGIT>', 'was', 'replaced', 'by', 'the', '4.3', 'L', '<2_DIGIT>', '-', 'degree', 'V', '6', '.']\n",
      "- to\n",
      "['Capsules', 'are', '0.4', '-', '0.5 mm', 'long', 'and', 'obovate', 'hemispheric', 'when', 'moist', 'becoming', 'obconic', 'when', 'dry', '.']\n",
      "- to\n",
      "['Presented', 'at', 'Cracow', 'Epiphany', 'Conference', 'on', 'Hadron', 'Spectroscopy', 'Cracow', 'Poland', '6', '-', '8', 'Jan 2005', '.']\n",
      "— sil\n",
      "['Buyers', 'Camp', 'Out', 'For', 'Condos', '—', 'Edmonton', 'Journal', 'September 24, 2006', '.']\n",
      "- to\n",
      "['Spanish', 'Language', 'in', 'the', 'Philippines', ':', '<YEAR>', '-', '<YEAR>', '.']\n",
      "— sil\n",
      "['Depth', '—', 'up', 'to', '<2_DIGIT>', 'meters', '.']\n",
      "- sil\n",
      "['ISBN', '0-9614392-2', '-', 'X', '.', 'The', 'Hershey', 'Archives', '\"\"', 'The', 'Hershey', 'Company', '\"\"', '.']\n",
      "- sil\n",
      "['Senators', 'Boxer', 'and', 'John', 'Ensign', '(', 'R', '-', 'NV', ')', 'are', 'the', 'authors', 'of', 'the', 'Invest', 'in', 'the', 'USA', 'Act', '.']\n",
      "- to\n",
      "['Bobby', 'Parks', 'and', 'Abet', 'Guidaben', 'combined', 'to', 'stretch', 'the', 'lead', 'to', '<2_DIGIT>', 'points', 'in', 'the', 'third', 'quarter', 'at', '<2_DIGIT>', '-', '<2_DIGIT>', '.']\n",
      "- to\n",
      "[\"Australia's\", 'Music', 'Charts', '<YEAR>', '-', '<YEAR>', '.']\n",
      "- sil\n",
      "['Moreover', 'etale', 'K', '-', 'theory', 'admitted', 'a', 'spectral', 'sequence', 'similar', 'to', 'the', 'one', 'conjectured', 'by', 'Quillen', '.']\n",
      "- sil\n",
      "['The', 'building', 'will', 'stand', '<2_DIGIT>', '-', 'stories', 'and', '<3_DIGIT>', 'feet', '.']\n",
      "- sil\n",
      "['Thus', 'a', 'humanly', 'perceived', 'color', 'may', 'be', 'thought', 'of', 'as', 'a', 'point', 'in', '3', '-', 'dimensional', 'Euclidean', 'space', '.']\n",
      "- to\n",
      "['Journal', 'of', 'the', 'American', 'Chemical', 'Society', '134.2', '(', '<YEAR>', ')', ':', '<3_DIGIT>', '-', '<3_DIGIT>', '.']\n",
      "- sil\n",
      "['The', 'Complete', 'Directory', 'to', 'Prime', 'Time', 'Network', 'and', 'Cable', 'TV', 'Shows', '<YEAR>', '-', 'Present', '.']\n",
      "- sil\n",
      "['Fountain', 'of', 'the', 'Sun', 'Country', 'Club', '(', 'Mesa', 'Arizona', ')', ';', 'designed', 'the', '<2_DIGIT>', '-', 'hole', 'course', 'in', '<YEAR>', '.']\n",
      "- sil\n",
      "['The', 'contrast', 'was', '2', '-', 'TO', '1', '.']\n",
      "— sil\n",
      "['\"\"', 'Arcade', 'Alley', ':', 'Lure', 'of', 'the', 'Labyrinth', '—', 'Exploring', 'Maze', 'Games', '\"\"', '.']\n",
      "- to\n",
      "['Pat', 'Scanlan', '(', '<YEAR>', '-', '<YEAR>', ')', 'was', 'the', 'managing', 'editor', '(', '<YEAR>', '-', '<YEAR>', ')', 'of', 'the', 'Brooklyn', 'Tablet', 'the', 'official', 'paper', 'of', 'the', 'Brooklyn', 'diocese', '.']\n",
      "— sil\n",
      "['PlantNET', '—', 'New', 'South', 'Wales', 'Flora', 'Online', '.']\n",
      "- sil\n",
      "['\"\"', 'Enacted', 'Education', 'Budget', '<YEAR>', '-', '<YEAR>', '\"\"', '.']\n",
      "- to\n",
      "['<3_DIGIT>', '-', '<3_DIGIT>', 'Article', '6.1', 'of', 'the', 'DSUArticle', '<2_DIGIT>', 'of', 'the', 'DSU', '.']\n",
      "- sil\n",
      "['It', 'was', 'first', 'mapped', 'by', 'Scottish', 'geologist', 'David', 'Ferguson', 'in', '<YEAR>', '-', '<2_DIGIT>', '.']\n",
      "- sil\n",
      "['Pennsylvania', 'Department', 'of', 'Education', 'Pennsylvania', 'SSAct', '1', '_', 'Act', '1', 'Exceptions', 'Report', '<YEAR>', '-', '<YEAR>', 'April 2010', 'Scarcella', 'Frank', 'and', 'Pursell', 'Tricia', '(', 'May 25, 2010', ')', '.']\n",
      "- sil\n",
      "['The', 'smell', 'of', 'the', 'freshly', 'cut', 'grass', 'is', 'produced', 'mainly', 'by', 'cis-', '3', '-', 'Hexenal', '.']\n",
      "- to\n",
      "['Boston', 'Globe', '(', '<YEAR>', '-', '<YEAR>', ')', 'pp', '.']\n",
      "- to\n",
      "['The', 'Standard', 'Catalog', 'of', 'American', 'Cars', '<YEAR>', '-', '<YEAR>', '.']\n",
      "— sil\n",
      "['\"\"', 'Darkthrone', '—', 'Circle', 'The', 'Wagons', 'review', '\"\"', '.']\n",
      "- to\n",
      "['P', '.', 'de', 'Silva', '19 April', '<YEAR>', '-', '5', 'December 1959', 'Sri', 'Lanka', 'Freedom', 'Party', '3', 'rdJ', '.']\n",
      "- to\n",
      "['R', '.', 'Jayewardene', '30 March', '<YEAR>', '-', '<2_DIGIT>', 'April 1960', 'United', 'National', 'Party', '4', 'thC', '.']\n",
      "- to\n",
      "['Budd', 'worked', 'for', 'five', 'seasons', 'as', 'a', 'company', 'member', 'at', \"Canada's\", 'Stratford', 'Festival', 'under', 'Robin', 'Phillips', '(', '<YEAR>', '-', '<YEAR>', ')', '.']\n",
      "- sil\n",
      "['Law', '#', '11/2013', '-', 'OZResolution', '#', '<3_DIGIT>', '-', 'PGLaw', '#', '95/2005', '-', 'OZRussian', 'Federal', 'State', 'Statistics', 'Service', '(', '<YEAR>', ')', '.']\n",
      "— sil\n",
      "['V.L.', 'Patil', '—', 'former', 'Revenue', 'Minister', 'Social', 'welfare', 'Minister', 'Transport', '&', 'Labour', 'Industry', 'Horticulture', 'and', 'Member', 'of', 'Parliament', '.']\n",
      "- to\n",
      "['China', 'and', 'the', 'International', 'System', '<YEAR>', '-', '<YEAR>', ':', 'Power', 'Presence', 'and', 'Perceptions', 'in', 'a', 'Century', 'of', 'Humiliation', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— sil\n",
      "['\"\"', 'Active', 'State', 'Medical', 'Marijuana', 'Programs', '—', 'NORML', '\"\"', '.']\n",
      "- to\n",
      "['The', 'program', 'was', 'funded', 'from', '<YEAR>', '-', '<YEAR>', '.']\n",
      "- sil\n",
      "['The', 'school', 'board', 'set', 'property', 'tax', 'rates', 'in', '<YEAR>', '-', '<YEAR>', 'at', '86.5000', 'mills', '.']\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for s in sentences:\n",
    "    p = -1\n",
    "    haihun = ''\n",
    "    if '-' in s[0]:\n",
    "        haihun='-'\n",
    "        p = s[0].index('-')\n",
    "    elif '—' in s[0]:\n",
    "        haihun='—'\n",
    "        p = s[0].index('—')\n",
    "    if 0<p<len(s[0])-1:\n",
    "        print(s[0][p],s[1][p])\n",
    "        print(s[0])\n",
    "        i+=1\n",
    "        if i==100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "match = re.search('-', 'aiu-eee-ooo-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', 'ca', 'ca']\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"ca\"\n",
    "text = \"caabsacasca\"\n",
    "# パターンにマッチしたすべてをリストとして返す\n",
    "matchedList = re.findall(pattern,text)\n",
    "if matchedList:\n",
    "    print(matchedList) # ['34567', '34567']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca\n",
      "0\n",
      "2\n",
      "(0, 2)\n",
      "ca\n",
      "6\n",
      "8\n",
      "(6, 8)\n",
      "ca\n",
      "9\n",
      "11\n",
      "(9, 11)\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"ca\"\n",
    "text = \"caabsacasca\"\n",
    "# パターンにマッチしたすべてをイテレータとして返す\n",
    "iterator = re.finditer(pattern ,text)\n",
    "for match in iterator:\n",
    "    print(match.group())   # 1回目: ca      2回目: ca   \n",
    "    print(match.start())   # 1回目: 0       2回目: 6      \n",
    "    print(match.end())     # 1回目: 2       2回目: 8      \n",
    "    print(match.span())    # 1回目: (0, 2)  2回目: (6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[-—]\"\n",
    "text = \"ai-aiu-aiuoe—\"\n",
    "text= \"aiueo\"\n",
    "# パターンにマッチしたすべてをイテレータとして返す\n",
    "iterator = re.finditer(pattern ,text)\n",
    "for match in iterator:\n",
    "    print(match.group())   # 1回目: ca      2回目: ca   \n",
    "    print(match.start())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
